import mnm._ffi.op.imp as ffi
from mnm._core.core_utils import set_module
from . import imp_utils

__all__ = ["add", "avg_pool2d", "avg_pool2d_dx", "batch_flatten", "batch_norm", "conv2d", "conv2d_dw", "conv2d_dx", "divide", "equal", "greater", "greater_equal", "less", "less_equal", "linear", "log_softmax", "log_softmax_dx", "logical_not", "max_pool2d", "max_pool2d_dx", "mod", "multiply", "negative", "not_equal", "relu", "relu_dx", "sigmoid", "sigmoid_dx", "softmax", "softmax_dx", "subtract", "tanh", "tanh_dx"]

@set_module("mnm")
def add(x1, x2, out=None, where=None):
    x1 = imp_utils.ToAny(x1)
    x2 = imp_utils.ToAny(x2)
    out = imp_utils.ToAny(out)
    where = imp_utils.ToAny(where)
    return imp_utils.Ret(ffi.add(x1, x2, out, where))
@set_module("mnm")
def avg_pool2d(x, kernel, stride=None, padding=0, dilation=1, ceil_mode=False, include_pad=True):
    x = imp_utils.ToTensor(x)
    kernel = imp_utils.ToIntTuple(kernel)
    stride = imp_utils.ToOptionalIntTuple(stride)
    padding = imp_utils.ToIntTuple(padding)
    dilation = imp_utils.ToIntTuple(dilation)
    ceil_mode = imp_utils.ToBool(ceil_mode)
    include_pad = imp_utils.ToBool(include_pad)
    return imp_utils.Ret(ffi.avg_pool2d(x, kernel, stride, padding, dilation, ceil_mode, include_pad))
@set_module("mnm")
def avg_pool2d_dx(x, y, dy, kernel, stride, padding, dilation, ceil_mode, include_pad):
    x = imp_utils.ToTensor(x)
    y = imp_utils.ToTensor(y)
    dy = imp_utils.ToTensor(dy)
    kernel = imp_utils.ToIntTuple(kernel)
    stride = imp_utils.ToIntTuple(stride)
    padding = imp_utils.ToIntTuple(padding)
    dilation = imp_utils.ToIntTuple(dilation)
    ceil_mode = imp_utils.ToBool(ceil_mode)
    include_pad = imp_utils.ToBool(include_pad)
    return imp_utils.Ret(ffi.avg_pool2d_dx(x, y, dy, kernel, stride, padding, dilation, ceil_mode, include_pad))
@set_module("mnm")
def batch_flatten(x):
    x = imp_utils.ToAny(x)
    return imp_utils.Ret(ffi.batch_flatten(x))
@set_module("mnm")
def batch_norm(x, running_mean, running_var, scale=None, bias=None, eps=1e-05, momentum=0.1):
    x = imp_utils.ToTensor(x)
    running_mean = imp_utils.ToTensor(running_mean)
    running_var = imp_utils.ToTensor(running_var)
    scale = imp_utils.ToTensor(scale)
    bias = imp_utils.ToTensor(bias)
    eps = imp_utils.ToDouble(eps)
    momentum = imp_utils.ToDouble(momentum)
    return imp_utils.Ret(ffi.batch_norm(x, running_mean, running_var, scale, bias, eps, momentum))
@set_module("mnm")
def conv2d(x, w, stride=1, padding=0, dilation=1, groups=1):
    x = imp_utils.ToTensor(x)
    w = imp_utils.ToTensor(w)
    stride = imp_utils.ToIntTuple(stride)
    padding = imp_utils.ToIntTuple(padding)
    dilation = imp_utils.ToIntTuple(dilation)
    groups = imp_utils.ToInt(groups)
    return imp_utils.Ret(ffi.conv2d(x, w, stride, padding, dilation, groups))
@set_module("mnm")
def conv2d_dw(x_or_w, y, dy, stride, padding, dilation, groups):
    x_or_w = imp_utils.ToTensor(x_or_w)
    y = imp_utils.ToTensor(y)
    dy = imp_utils.ToTensor(dy)
    stride = imp_utils.ToIntTuple(stride)
    padding = imp_utils.ToIntTuple(padding)
    dilation = imp_utils.ToIntTuple(dilation)
    groups = imp_utils.ToInt(groups)
    return imp_utils.Ret(ffi.conv2d_dw(x_or_w, y, dy, stride, padding, dilation, groups))
@set_module("mnm")
def conv2d_dx(x_or_w, y, dy, stride, padding, dilation, groups):
    x_or_w = imp_utils.ToTensor(x_or_w)
    y = imp_utils.ToTensor(y)
    dy = imp_utils.ToTensor(dy)
    stride = imp_utils.ToIntTuple(stride)
    padding = imp_utils.ToIntTuple(padding)
    dilation = imp_utils.ToIntTuple(dilation)
    groups = imp_utils.ToInt(groups)
    return imp_utils.Ret(ffi.conv2d_dx(x_or_w, y, dy, stride, padding, dilation, groups))
@set_module("mnm")
def divide(x1, x2, out=None, where=None):
    x1 = imp_utils.ToAny(x1)
    x2 = imp_utils.ToAny(x2)
    out = imp_utils.ToAny(out)
    where = imp_utils.ToAny(where)
    return imp_utils.Ret(ffi.divide(x1, x2, out, where))
@set_module("mnm")
def equal(x1, x2, out=None, where=None):
    x1 = imp_utils.ToAny(x1)
    x2 = imp_utils.ToAny(x2)
    out = imp_utils.ToAny(out)
    where = imp_utils.ToAny(where)
    return imp_utils.Ret(ffi.equal(x1, x2, out, where))
@set_module("mnm")
def greater(x1, x2, out=None, where=None):
    x1 = imp_utils.ToAny(x1)
    x2 = imp_utils.ToAny(x2)
    out = imp_utils.ToAny(out)
    where = imp_utils.ToAny(where)
    return imp_utils.Ret(ffi.greater(x1, x2, out, where))
@set_module("mnm")
def greater_equal(x1, x2, out=None, where=None):
    x1 = imp_utils.ToAny(x1)
    x2 = imp_utils.ToAny(x2)
    out = imp_utils.ToAny(out)
    where = imp_utils.ToAny(where)
    return imp_utils.Ret(ffi.greater_equal(x1, x2, out, where))
@set_module("mnm")
def less(x1, x2, out=None, where=None):
    x1 = imp_utils.ToAny(x1)
    x2 = imp_utils.ToAny(x2)
    out = imp_utils.ToAny(out)
    where = imp_utils.ToAny(where)
    return imp_utils.Ret(ffi.less(x1, x2, out, where))
@set_module("mnm")
def less_equal(x1, x2, out=None, where=None):
    x1 = imp_utils.ToAny(x1)
    x2 = imp_utils.ToAny(x2)
    out = imp_utils.ToAny(out)
    where = imp_utils.ToAny(where)
    return imp_utils.Ret(ffi.less_equal(x1, x2, out, where))
@set_module("mnm")
def linear(x1, x2):
    x1 = imp_utils.ToAny(x1)
    x2 = imp_utils.ToAny(x2)
    return imp_utils.Ret(ffi.linear(x1, x2))
@set_module("mnm")
def log_softmax(x, axis=-1):
    x = imp_utils.ToTensor(x)
    axis = imp_utils.ToInt(axis)
    return imp_utils.Ret(ffi.log_softmax(x, axis))
@set_module("mnm")
def log_softmax_dx(x, y, dy, axis=-1):
    x = imp_utils.ToTensor(x)
    y = imp_utils.ToTensor(y)
    dy = imp_utils.ToTensor(dy)
    axis = imp_utils.ToInt(axis)
    return imp_utils.Ret(ffi.log_softmax_dx(x, y, dy, axis))
@set_module("mnm")
def logical_not(x, out=None, where=None):
    x = imp_utils.ToAny(x)
    out = imp_utils.ToAny(out)
    where = imp_utils.ToAny(where)
    return imp_utils.Ret(ffi.logical_not(x, out, where))
@set_module("mnm")
def max_pool2d(x, kernel, stride=None, padding=0, dilation=1, ceil_mode=False, include_pad=True):
    x = imp_utils.ToTensor(x)
    kernel = imp_utils.ToIntTuple(kernel)
    stride = imp_utils.ToOptionalIntTuple(stride)
    padding = imp_utils.ToIntTuple(padding)
    dilation = imp_utils.ToIntTuple(dilation)
    ceil_mode = imp_utils.ToBool(ceil_mode)
    include_pad = imp_utils.ToBool(include_pad)
    return imp_utils.Ret(ffi.max_pool2d(x, kernel, stride, padding, dilation, ceil_mode, include_pad))
@set_module("mnm")
def max_pool2d_dx(x, y, dy, kernel, stride, padding, dilation, ceil_mode, include_pad):
    x = imp_utils.ToTensor(x)
    y = imp_utils.ToTensor(y)
    dy = imp_utils.ToTensor(dy)
    kernel = imp_utils.ToIntTuple(kernel)
    stride = imp_utils.ToIntTuple(stride)
    padding = imp_utils.ToIntTuple(padding)
    dilation = imp_utils.ToIntTuple(dilation)
    ceil_mode = imp_utils.ToBool(ceil_mode)
    include_pad = imp_utils.ToBool(include_pad)
    return imp_utils.Ret(ffi.max_pool2d_dx(x, y, dy, kernel, stride, padding, dilation, ceil_mode, include_pad))
@set_module("mnm")
def mod(x1, x2, out=None, where=None):
    x1 = imp_utils.ToAny(x1)
    x2 = imp_utils.ToAny(x2)
    out = imp_utils.ToAny(out)
    where = imp_utils.ToAny(where)
    return imp_utils.Ret(ffi.mod(x1, x2, out, where))
@set_module("mnm")
def multiply(x1, x2, out=None, where=None):
    x1 = imp_utils.ToAny(x1)
    x2 = imp_utils.ToAny(x2)
    out = imp_utils.ToAny(out)
    where = imp_utils.ToAny(where)
    return imp_utils.Ret(ffi.multiply(x1, x2, out, where))
@set_module("mnm")
def negative(x, out=None, where=None):
    x = imp_utils.ToAny(x)
    out = imp_utils.ToAny(out)
    where = imp_utils.ToAny(where)
    return imp_utils.Ret(ffi.negative(x, out, where))
@set_module("mnm")
def not_equal(x1, x2, out=None, where=None):
    x1 = imp_utils.ToAny(x1)
    x2 = imp_utils.ToAny(x2)
    out = imp_utils.ToAny(out)
    where = imp_utils.ToAny(where)
    return imp_utils.Ret(ffi.not_equal(x1, x2, out, where))
@set_module("mnm")
def relu(x):
    x = imp_utils.ToAny(x)
    return imp_utils.Ret(ffi.relu(x))
@set_module("mnm")
def relu_dx(x, y, dy):
    x = imp_utils.ToAny(x)
    y = imp_utils.ToTensor(y)
    dy = imp_utils.ToTensor(dy)
    return imp_utils.Ret(ffi.relu_dx(x, y, dy))
@set_module("mnm")
def sigmoid(x):
    x = imp_utils.ToAny(x)
    return imp_utils.Ret(ffi.sigmoid(x))
@set_module("mnm")
def sigmoid_dx(x, y, dy):
    x = imp_utils.ToAny(x)
    y = imp_utils.ToTensor(y)
    dy = imp_utils.ToTensor(dy)
    return imp_utils.Ret(ffi.sigmoid_dx(x, y, dy))
@set_module("mnm")
def softmax(x, axis=-1):
    x = imp_utils.ToTensor(x)
    axis = imp_utils.ToInt(axis)
    return imp_utils.Ret(ffi.softmax(x, axis))
@set_module("mnm")
def softmax_dx(x, y, dy, axis=-1):
    x = imp_utils.ToTensor(x)
    y = imp_utils.ToTensor(y)
    dy = imp_utils.ToTensor(dy)
    axis = imp_utils.ToInt(axis)
    return imp_utils.Ret(ffi.softmax_dx(x, y, dy, axis))
@set_module("mnm")
def subtract(x1, x2, out=None, where=None):
    x1 = imp_utils.ToAny(x1)
    x2 = imp_utils.ToAny(x2)
    out = imp_utils.ToAny(out)
    where = imp_utils.ToAny(where)
    return imp_utils.Ret(ffi.subtract(x1, x2, out, where))
@set_module("mnm")
def tanh(x):
    x = imp_utils.ToAny(x)
    return imp_utils.Ret(ffi.tanh(x))
@set_module("mnm")
def tanh_dx(x, y, dy):
    x = imp_utils.ToAny(x)
    y = imp_utils.ToTensor(y)
    dy = imp_utils.ToTensor(dy)
    return imp_utils.Ret(ffi.tanh_dx(x, y, dy))
